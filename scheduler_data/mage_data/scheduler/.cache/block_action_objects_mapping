{"block_file": {"data_exporters/export_customers_data.py:data_exporter:python:export customers data": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom os import path\n\nimport pandas as pd\nfrom psycopg2.extras import execute_values\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\n\nCREATE TABLE IF NOT EXISTS raw.qb_customers (\n    id TEXT PRIMARY KEY,\n    payload JSONB NOT NULL,\n    ingested_at_utc TIMESTAMPTZ NOT NULL,\n    extract_window_start_utc TIMESTAMPTZ NOT NULL,\n    extract_window_end_utc TIMESTAMPTZ NOT NULL,\n    page_number INTEGER NOT NULL,\n    page_size INTEGER NOT NULL,\n    request_payload JSONB NOT NULL\n);\n\"\"\"\n\nUPSERT_SQL = \"\"\"\nINSERT INTO raw.qb_customers (\n    id,\n    payload,\n    ingested_at_utc,\n    extract_window_start_utc,\n    extract_window_end_utc,\n    page_number,\n    page_size,\n    request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE SET\n    payload = EXCLUDED.payload,\n    ingested_at_utc = EXCLUDED.ingested_at_utc,\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload;\n\"\"\"\n\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs) -> dict:\n    \"\"\"\n    Espera un DF desde Transform con columnas:\n      - id\n      - payload_json\n      - ingested_at_utc\n      - extract_window_start_utc\n      - extract_window_end_utc\n      - page_number\n      - page_size\n      - request_payload_json\n    \"\"\"\n    if df is None or df.empty:\n        return {\"rows_loaded\": 0}\n\n    schema_name = \"raw\"\n    table_name = \"qb_customers\"\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n\n    rows = list(\n        df[\n            [\n                \"id\",\n                \"payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n                \"request_payload_json\",\n            ]\n        ].itertuples(index=False, name=None)\n    )\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Acceso al cursor/conn de psycopg2 por debajo\n        conn = loader.conn\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n\n            template = (\n                \"(\"\n                \"%s, \"\n                \"%s::jsonb, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s, \"\n                \"%s, \"\n                \"%s::jsonb\"\n                \")\"\n            )\n\n            execute_values(cur, UPSERT_SQL, rows, template=template, page_size=1000)\n\n        conn.commit()\n\n    return {\"rows_loaded\": len(rows), \"target\": f\"{schema_name}.{table_name}\"}\n", "file_path": "data_exporters/export_customers_data.py", "language": "python", "type": "data_exporter", "uuid": "export_customers_data"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_loaders/load_customers_data.py:data_loader:python:load customers data": {"content": "import time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import List, Dict, Any, Tuple\n\nimport pandas as pd\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif \"data_loader\" not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\n\ndef get_base_url() -> str:\n    env = get_secret_value(\"qbo_env\")\n    if str(env).lower() == \"sandbox\":\n        return \"https://sandbox-quickbooks.api.intuit.com\"\n    return \"https://quickbooks.api.intuit.com\"\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value(\"qbo_client_id\")\n    client_secret = get_secret_value(\"qbo_client_secret\")\n    refresh_token = get_secret_value(\"qbo_refresh_token\")\n\n    response = requests.post(\n        TOKEN_URL,\n        auth=requests.auth.HTTPBasicAuth(client_id, client_secret),\n        data={\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token},\n        timeout=30,\n    )\n    response.raise_for_status()\n    return response.json()[\"access_token\"]\n\n\ndef request_get_with_retries(\n    url: str,\n    headers: Dict[str, str],\n    params: Dict[str, Any],\n    max_retries: int = 5\n) -> Dict[str, Any]:\n    for attempt in range(max_retries):\n        r = requests.get(url, headers=headers, params=params, timeout=60)\n\n        if r.status_code in (429, 500, 502, 503, 504):\n            time.sleep(2 ** attempt)\n            continue\n\n        r.raise_for_status()\n        return r.json()\n\n    raise RuntimeError(\"QuickBooks request fall\u00f3 tras varios reintentos.\")\n\n\ndef format_qb_datetime(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n\ndef split_date_range(start_utc: datetime, end_utc: datetime, chunk_days: int) -> List[Tuple[datetime, datetime]]:\n    windows = []\n    current = start_utc\n    while current < end_utc:\n        nxt = min(current + timedelta(days=chunk_days), end_utc)\n        windows.append((current, nxt))\n        current = nxt\n    return windows\n\n\ndef fetch_customers_window(\n    realm_id: str,\n    base_url: str,\n    access_token: str,\n    start_dt: datetime,\n    end_dt: datetime,\n    minor_version: int = 75,\n    page_size: int = 1000,\n) -> List[Dict[str, Any]]:\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    start_str = format_qb_datetime(start_dt)\n    end_str = format_qb_datetime(end_dt)\n\n    start_position = 1\n    page_number = 1\n    rows: List[Dict[str, Any]] = []\n\n    while True:\n        query = (\n            \"SELECT * FROM Customer \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_str}' \"\n            f\"AND MetaData.LastUpdatedTime < '{end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        params = {\"query\": query, \"minorversion\": minor_version}\n        data = request_get_with_retries(url, headers, params)\n\n        if \"Fault\" in data:\n            break\n\n        customers = data.get(\"QueryResponse\", {}).get(\"Customer\", [])\n        if not customers:\n            break\n\n        request_payload = {\"query\": query, \"minorversion\": minor_version}\n\n        for c in customers:\n            rows.append(\n                {\n                    \"id\": str(c.get(\"Id\")) if c.get(\"Id\") is not None else None,\n                    \"payload\": c,\n                    \"extract_window_start_utc\": start_str,\n                    \"extract_window_end_utc\": end_str,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                }\n            )\n\n        if len(customers) < page_size:\n            break\n\n        start_position += page_size\n        page_number += 1\n        time.sleep(0.3)\n\n    return rows\n\n\n@data_loader\ndef load_data(*args, **kwargs) -> pd.DataFrame:\n    fecha_inicio = kwargs.get(\"fecha_inicio\")\n    fecha_fin = kwargs.get(\"fecha_fin\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debes pasar fecha_inicio y fecha_fin, ej: '2024-01-01T00:00:00Z'.\")\n\n    chunk_days = int(kwargs.get(\"chunk_days\", 30))\n\n    start_utc = datetime.fromisoformat(fecha_inicio.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n    end_utc = datetime.fromisoformat(fecha_fin.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n\n    realm_id = get_secret_value(\"qbo_realm_id\")\n    base_url = get_base_url()\n    access_token = get_access_token()\n\n    windows = split_date_range(start_utc, end_utc, chunk_days)\n\n    all_rows: List[Dict[str, Any]] = []\n    for w_start, w_end in windows:\n        all_rows.extend(\n            fetch_customers_window(\n                realm_id=realm_id,\n                base_url=base_url,\n                access_token=access_token,\n                start_dt=w_start,\n                end_dt=w_end,\n            )\n        )\n\n    df = pd.DataFrame(all_rows)\n    #print(df.shape)\n    #print(df.columns)\n    #print(df.head(5))\n    return df\n", "file_path": "data_loaders/load_customers_data.py", "language": "python", "type": "data_loader", "uuid": "load_customers_data"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/transform_customers_data.py:transformer:python:transform customers data": {"content": "import json\nfrom datetime import datetime, timezone\n\nimport pandas as pd\n\nif \"transformer\" not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Input (desde Extract):\n      - id\n      - payload\n      - extract_window_start_utc\n      - extract_window_end_utc\n      - page_number\n      - page_size\n      - request_payload\n\n    Output (para RAW Load):\n      - id\n      - payload_json\n      - request_payload_json\n      - ingested_at_utc\n      - extract_window_start_utc\n      - extract_window_end_utc\n      - page_number\n      - page_size\n    \"\"\"\n    if df is None or df.empty:\n        return pd.DataFrame(\n            columns=[\n                \"id\",\n                \"payload_json\",\n                \"request_payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n            ]\n        )\n\n    df = df.copy()\n\n    # id obligatorio\n    df = df[df[\"id\"].notna()]\n    df[\"id\"] = df[\"id\"].astype(str)\n\n    # timestamps de ingesta (UTC)\n    ingested_at = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    df[\"ingested_at_utc\"] = ingested_at\n\n    # JSON listo para Postgres jsonb\n    df[\"payload_json\"] = df[\"payload\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n    df[\"request_payload_json\"] = df[\"request_payload\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n\n    out = df[\n        [\n            \"id\",\n            \"payload_json\",\n            \"request_payload_json\",\n            \"ingested_at_utc\",\n            \"extract_window_start_utc\",\n            \"extract_window_end_utc\",\n            \"page_number\",\n            \"page_size\",\n        ]\n    ].reset_index(drop=True)\n\n    #print(type(out))\n    #print(out.columns)\n    return out\n", "file_path": "transformers/transform_customers_data.py", "language": "python", "type": "transformer", "uuid": "transform_customers_data"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_path: data_loaders/load_customers_data.py\n    file_source:\n      path: data_loaders/load_customers_data.py\n  downstream_blocks:\n  - transform_customers_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_customers_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_customers_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_customers_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: transform_customers_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - load_customers_data\n  uuid: transform_customers_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_customers_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - transform_customers_data\n  uuid: export_customers_data\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-27 14:44:19.454923+00:00'\ndata_integration: null\ndescription: ''\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks: []\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-01-27 14:43:58.682882+00:00'\ndata_integration: null\ndescription: ''\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "created_at: '2026-01-27 14:44:40.785152+00:00'\ndescription: null\nname: qb_items_backfill\ntags: []\ntype: python\nuuid: qb_items_backfill\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}, "/home/src/scheduler/data_loaders/load_customers_data.py:data_loader:python:home/src/scheduler/data loaders/load customers data": {"content": "import time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import List, Dict, Any, Tuple\n\nimport pandas as pd\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif \"data_loader\" not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\ndef get_base_url() -> str:\n    env = get_secret_value(\"qbo_env\")\n    if str(env).lower() == \"sandbox\":\n        return \"https://sandbox-quickbooks.api.intuit.com\"\n    return \"https://quickbooks.api.intuit.com\"\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value(\"qbo_client_id\")\n    client_secret = get_secret_value(\"qbo_client_secret\")\n    refresh_token = get_secret_value(\"qbo_refresh_token\")\n\n    log(\"Pidiendo access token a Intuit...\")\n    response = requests.post(\n        TOKEN_URL,\n        auth=requests.auth.HTTPBasicAuth(client_id, client_secret),\n        data={\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token},\n        timeout=30,\n    )\n    response.raise_for_status()\n    log(\"Access token obtenido.\")\n    return response.json()[\"access_token\"]\n\n\ndef request_get_with_retries(\n    url: str,\n    headers: Dict[str, str],\n    params: Dict[str, Any],\n    max_retries: int = 5\n) -> Dict[str, Any]:\n    for attempt in range(max_retries):\n        r = requests.get(url, headers=headers, params=params, timeout=60)\n\n        if r.status_code in (429, 500, 502, 503, 504):\n            wait_s = 2 ** attempt\n            log(f\"HTTP {r.status_code}. Reintento {attempt + 1}/{max_retries} en {wait_s}s...\")\n            time.sleep(wait_s)\n            continue\n\n        r.raise_for_status()\n        return r.json()\n\n    raise RuntimeError(\"QuickBooks request fall\u00f3 tras varios reintentos.\")\n\n\ndef format_qb_datetime(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n\ndef split_date_range(start_utc: datetime, end_utc: datetime, chunk_days: int) -> List[Tuple[datetime, datetime]]:\n    windows = []\n    current = start_utc\n    while current < end_utc:\n        nxt = min(current + timedelta(days=chunk_days), end_utc)\n        windows.append((current, nxt))\n        current = nxt\n    return windows\n\n\ndef fetch_customers_window(\n    realm_id: str,\n    base_url: str,\n    access_token: str,\n    start_dt: datetime,\n    end_dt: datetime,\n    minor_version: int = 75,\n    page_size: int = 1000,\n) -> List[Dict[str, Any]]:\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    start_str = format_qb_datetime(start_dt)\n    end_str = format_qb_datetime(end_dt)\n\n    log(f\"Extrayendo ventana: {start_str} -> {end_str}\")\n\n    start_position = 1\n    page_number = 1\n    rows: List[Dict[str, Any]] = []\n\n    while True:\n        query = (\n            \"SELECT * FROM Customer \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_str}' \"\n            f\"AND MetaData.LastUpdatedTime < '{end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        params = {\"query\": query, \"minorversion\": minor_version}\n        data = request_get_with_retries(url, headers, params)\n\n        if \"Fault\" in data:\n            log(\"QuickBooks respondi\u00f3 con 'Fault'. Se omite esta ventana.\")\n            break\n\n        customers = data.get(\"QueryResponse\", {}).get(\"Customer\", [])\n        if not customers:\n            if page_number == 1:\n                log(\"Ventana sin resultados.\")\n            break\n\n        log(f\"  P\u00e1gina {page_number}: {len(customers)} customers\")\n\n        request_payload = {\"query\": query, \"minorversion\": minor_version}\n\n        for c in customers:\n            rows.append(\n                {\n                    \"id\": str(c.get(\"Id\")) if c.get(\"Id\") is not None else None,\n                    \"payload\": c,\n                    \"extract_window_start_utc\": start_str,\n                    \"extract_window_end_utc\": end_str,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                }\n            )\n\n        if len(customers) < page_size:\n            break\n\n        start_position += page_size\n        page_number += 1\n        time.sleep(0.3)\n\n    log(f\"Ventana terminada. Filas acumuladas: {len(rows)}\")\n    return rows\n\n\n@data_loader\ndef load_data(*args, **kwargs) -> pd.DataFrame:\n    fecha_inicio = kwargs.get(\"fecha_inicio\")\n    fecha_fin = kwargs.get(\"fecha_fin\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debes pasar fecha_inicio y fecha_fin, ej: '2024-01-01T00:00:00Z'.\")\n\n    chunk_days = int(kwargs.get(\"chunk_days\", 30))\n\n    start_utc = datetime.fromisoformat(fecha_inicio.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n    end_utc = datetime.fromisoformat(fecha_fin.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n\n    log(\"Inicio Extract Customers\")\n    log(f\"Rango: {fecha_inicio} -> {fecha_fin} | chunk_days={chunk_days}\")\n\n    realm_id = get_secret_value(\"qbo_realm_id\")\n    base_url = get_base_url()\n    access_token = get_access_token()\n\n    windows = split_date_range(start_utc, end_utc, chunk_days)\n    log(f\"Total ventanas a procesar: {len(windows)}\")\n\n    all_rows: List[Dict[str, Any]] = []\n    for i, (w_start, w_end) in enumerate(windows, start=1):\n        log(f\"Procesando ventana {i}/{len(windows)}\")\n        all_rows.extend(\n            fetch_customers_window(\n                realm_id=realm_id,\n                base_url=base_url,\n                access_token=access_token,\n                start_dt=w_start,\n                end_dt=w_end,\n            )\n        )\n\n    df = pd.DataFrame(all_rows)\n    log(f\"Extract terminado. Total filas: {len(df)}\")\n    return df\n", "file_path": "/home/src/scheduler/data_loaders/load_customers_data.py", "language": "python", "type": "data_loader", "uuid": "load_customers_data"}, "/home/src/scheduler/transformers/transform_customers_data.py:transformer:python:home/src/scheduler/transformers/transform customers data": {"content": "import json\nfrom datetime import datetime, timezone\n\nimport pandas as pd\n\nif \"transformer\" not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    if df is None or df.empty:\n        log(\"Transform: DataFrame vac\u00edo.\")\n        return pd.DataFrame(\n            columns=[\n                \"id\",\n                \"payload_json\",\n                \"request_payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n            ]\n        )\n\n    log(f\"Transform: filas recibidas = {len(df)}\")\n\n    df = df.copy()\n\n    # id obligatorio\n    df = df[df[\"id\"].notna()]\n    df[\"id\"] = df[\"id\"].astype(str)\n\n    # ingesta (UTC)\n    df[\"ingested_at_utc\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n    # JSON listo para jsonb\n    df[\"payload_json\"] = df[\"payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n    df[\"request_payload_json\"] = df[\"request_payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n\n    # asegurar tipos\n    df[\"page_number\"] = pd.to_numeric(df[\"page_number\"], errors=\"coerce\").fillna(0).astype(int)\n    df[\"page_size\"] = pd.to_numeric(df[\"page_size\"], errors=\"coerce\").fillna(0).astype(int)\n\n    out = df[\n        [\n            \"id\",\n            \"payload_json\",\n            \"request_payload_json\",\n            \"ingested_at_utc\",\n            \"extract_window_start_utc\",\n            \"extract_window_end_utc\",\n            \"page_number\",\n            \"page_size\",\n        ]\n    ].reset_index(drop=True)\n\n    log(f\"Transform: filas de salida = {len(out)}\")\n    return out\n", "file_path": "/home/src/scheduler/transformers/transform_customers_data.py", "language": "python", "type": "transformer", "uuid": "transform_customers_data"}, "/home/src/scheduler/data_exporters/export_customers_data.py:data_exporter:python:home/src/scheduler/data exporters/export customers data": {"content": "from datetime import datetime, timezone\nfrom os import path\n\nimport pandas as pd\nfrom psycopg2.extras import execute_values\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\n\nCREATE TABLE IF NOT EXISTS raw.qb_customers (\n    id TEXT PRIMARY KEY,\n    payload JSONB NOT NULL,\n    ingested_at_utc TIMESTAMPTZ NOT NULL,\n    extract_window_start_utc TIMESTAMPTZ NOT NULL,\n    extract_window_end_utc TIMESTAMPTZ NOT NULL,\n    page_number INTEGER NOT NULL,\n    page_size INTEGER NOT NULL,\n    request_payload JSONB NOT NULL\n);\n\"\"\"\n\nUPSERT_SQL = \"\"\"\nINSERT INTO raw.qb_customers (\n    id,\n    payload,\n    ingested_at_utc,\n    extract_window_start_utc,\n    extract_window_end_utc,\n    page_number,\n    page_size,\n    request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE SET\n    payload = EXCLUDED.payload,\n    ingested_at_utc = EXCLUDED.ingested_at_utc,\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload;\n\"\"\"\n\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs) -> dict:\n    if df is None or df.empty:\n        log(\"Load: DataFrame vac\u00edo. No se carga nada.\")\n        return {\"rows_loaded\": 0}\n\n    required_cols = [\n        \"id\",\n        \"payload_json\",\n        \"ingested_at_utc\",\n        \"extract_window_start_utc\",\n        \"extract_window_end_utc\",\n        \"page_number\",\n        \"page_size\",\n        \"request_payload_json\",\n    ]\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Load: faltan columnas requeridas: {missing}\")\n\n    log(f\"Load: filas recibidas = {len(df)}\")\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n\n    rows = list(\n        df[\n            [\n                \"id\",\n                \"payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n                \"request_payload_json\",\n            ]\n        ].itertuples(index=False, name=None)\n    )\n\n    log(\"Load: abriendo conexi\u00f3n a Postgres...\")\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Algunas versiones exponen conn como loader.conn, otras diferente:\n        conn = getattr(loader, \"conn\", None) or getattr(loader, \"_conn\", None) or getattr(loader, \"_connection\", None)\n        if conn is None:\n            raise AttributeError(\"Load: no pude obtener la conexi\u00f3n desde el loader (conn/_conn/_connection).\")\n\n        log(\"Load: conexi\u00f3n abierta. Creando tabla si no existe...\")\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n\n            template = (\n                \"(\"\n                \"%s, \"\n                \"%s::jsonb, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s, \"\n                \"%s, \"\n                \"%s::jsonb\"\n                \")\"\n            )\n\n            log(\"Load: ejecutando upsert...\")\n            execute_values(cur, UPSERT_SQL, rows, template=template, page_size=1000)\n\n        conn.commit()\n        log(\"Load: commit realizado.\")\n\n    log(\"Load: terminado.\")\n    return {\"rows_loaded\": len(rows), \"target\": \"raw.qb_customers\"}\n", "file_path": "/home/src/scheduler/data_exporters/export_customers_data.py", "language": "python", "type": "data_exporter", "uuid": "export_customers_data"}, "/home/src/scheduler/data_loaders/load_invoices_data.py:data_loader:python:home/src/scheduler/data loaders/load invoices data": {"content": "import time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import List, Dict, Any, Tuple\n\nimport pandas as pd\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif \"data_loader\" not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\ndef get_base_url() -> str:\n    env = get_secret_value(\"qbo_env\")\n    if str(env).lower() == \"sandbox\":\n        return \"https://sandbox-quickbooks.api.intuit.com\"\n    return \"https://quickbooks.api.intuit.com\"\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value(\"qbo_client_id\")\n    client_secret = get_secret_value(\"qbo_client_secret\")\n    refresh_token = get_secret_value(\"qbo_refresh_token\")\n\n    log(\"Pidiendo access token a Intuit...\")\n    response = requests.post(\n        TOKEN_URL,\n        auth=requests.auth.HTTPBasicAuth(client_id, client_secret),\n        data={\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token},\n        timeout=30,\n    )\n    response.raise_for_status()\n    log(\"Access token obtenido.\")\n    return response.json()[\"access_token\"]\n\n\ndef request_get_with_retries(\n    url: str,\n    headers: Dict[str, str],\n    params: Dict[str, Any],\n    max_retries: int = 5\n) -> Dict[str, Any]:\n    for attempt in range(max_retries):\n        r = requests.get(url, headers=headers, params=params, timeout=60)\n\n        if r.status_code in (429, 500, 502, 503, 504):\n            wait_s = 2 ** attempt\n            log(f\"HTTP {r.status_code}. Reintento {attempt + 1}/{max_retries} en {wait_s}s...\")\n            time.sleep(wait_s)\n            continue\n\n        r.raise_for_status()\n        return r.json()\n\n    raise RuntimeError(\"QuickBooks request fall\u00f3 tras varios reintentos.\")\n\n\ndef format_qb_datetime(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n\ndef split_date_range(start_utc: datetime, end_utc: datetime, chunk_days: int) -> List[Tuple[datetime, datetime]]:\n    windows = []\n    current = start_utc\n    while current < end_utc:\n        nxt = min(current + timedelta(days=chunk_days), end_utc)\n        windows.append((current, nxt))\n        current = nxt\n    return windows\n\n\ndef fetch_invoices_window(\n    realm_id: str,\n    base_url: str,\n    access_token: str,\n    start_dt: datetime,\n    end_dt: datetime,\n    minor_version: int = 75,\n    page_size: int = 1000,\n) -> List[Dict[str, Any]]:\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    start_str = format_qb_datetime(start_dt)\n    end_str = format_qb_datetime(end_dt)\n\n    log(f\"Extrayendo ventana: {start_str} -> {end_str}\")\n\n    start_position = 1\n    page_number = 1\n    rows: List[Dict[str, Any]] = []\n\n    while True:\n        query = (\n            \"SELECT * FROM Invoice \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_str}' \"\n            f\"AND MetaData.LastUpdatedTime < '{end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        params = {\"query\": query, \"minorversion\": minor_version}\n        data = request_get_with_retries(url, headers, params)\n\n        if \"Fault\" in data:\n            log(\"QuickBooks respondi\u00f3 con 'Fault'. Se omite esta ventana.\")\n            break\n\n        invoices = data.get(\"QueryResponse\", {}).get(\"Invoice\", [])\n        if not invoices:\n            if page_number == 1:\n                log(\"Ventana sin resultados.\")\n            break\n\n        log(f\"  P\u00e1gina {page_number}: {len(invoices)} invoices\")\n\n        request_payload = {\"query\": query, \"minorversion\": minor_version}\n\n        for inv in invoices:\n            rows.append(\n                {\n                    \"id\": str(inv.get(\"Id\")) if inv.get(\"Id\") is not None else None,\n                    \"payload\": inv,\n                    \"extract_window_start_utc\": start_str,\n                    \"extract_window_end_utc\": end_str,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                }\n            )\n\n        if len(invoices) < page_size:\n            break\n\n        start_position += page_size\n        page_number += 1\n        time.sleep(0.3)\n\n    log(f\"Ventana terminada. Filas acumuladas: {len(rows)}\")\n    return rows\n\n\n@data_loader\ndef load_data(*args, **kwargs) -> pd.DataFrame:\n    fecha_inicio = kwargs.get(\"fecha_inicio\")\n    fecha_fin = kwargs.get(\"fecha_fin\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debes pasar fecha_inicio y fecha_fin, ej: '2024-01-01T00:00:00Z'.\")\n\n    chunk_days = int(kwargs.get(\"chunk_days\", 30))\n\n    start_utc = datetime.fromisoformat(fecha_inicio.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n    end_utc = datetime.fromisoformat(fecha_fin.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n\n    log(\"Inicio Extract Invoices\")\n    log(f\"Rango: {fecha_inicio} -> {fecha_fin} | chunk_days={chunk_days}\")\n\n    realm_id = get_secret_value(\"qbo_realm_id\")\n    base_url = get_base_url()\n    access_token = get_access_token()\n\n    windows = split_date_range(start_utc, end_utc, chunk_days)\n    log(f\"Total ventanas a procesar: {len(windows)}\")\n\n    all_rows: List[Dict[str, Any]] = []\n    for i, (w_start, w_end) in enumerate(windows, start=1):\n        log(f\"Procesando ventana {i}/{len(windows)}\")\n        all_rows.extend(\n            fetch_invoices_window(\n                realm_id=realm_id,\n                base_url=base_url,\n                access_token=access_token,\n                start_dt=w_start,\n                end_dt=w_end,\n            )\n        )\n\n    df = pd.DataFrame(all_rows)\n    log(f\"Extract terminado. Total filas: {len(df)}\")\n    return df\n", "file_path": "/home/src/scheduler/data_loaders/load_invoices_data.py", "language": "python", "type": "data_loader", "uuid": "load_invoices_data"}, "/home/src/scheduler/transformers/transform_invoices_data.py:transformer:python:home/src/scheduler/transformers/transform invoices data": {"content": "import json\nfrom datetime import datetime, timezone\n\nimport pandas as pd\n\nif \"transformer\" not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    if df is None or df.empty:\n        log(\"Transform: DataFrame vac\u00edo.\")\n        return pd.DataFrame(\n            columns=[\n                \"id\",\n                \"payload_json\",\n                \"request_payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n            ]\n        )\n\n    log(f\"Transform: filas recibidas = {len(df)}\")\n\n    df = df.copy()\n\n    # id obligatorio\n    df = df[df[\"id\"].notna()]\n    df[\"id\"] = df[\"id\"].astype(str)\n\n    # ingesta (UTC)\n    df[\"ingested_at_utc\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n    # JSON listo para jsonb\n    df[\"payload_json\"] = df[\"payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n    df[\"request_payload_json\"] = df[\"request_payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n\n    # asegurar tipos\n    df[\"page_number\"] = pd.to_numeric(df[\"page_number\"], errors=\"coerce\").fillna(0).astype(int)\n    df[\"page_size\"] = pd.to_numeric(df[\"page_size\"], errors=\"coerce\").fillna(0).astype(int)\n\n    out = df[\n        [\n            \"id\",\n            \"payload_json\",\n            \"request_payload_json\",\n            \"ingested_at_utc\",\n            \"extract_window_start_utc\",\n            \"extract_window_end_utc\",\n            \"page_number\",\n            \"page_size\",\n        ]\n    ].reset_index(drop=True)\n\n    log(f\"Transform: filas de salida = {len(out)}\")\n    return out", "file_path": "/home/src/scheduler/transformers/transform_invoices_data.py", "language": "python", "type": "transformer", "uuid": "transform_invoices_data"}, "/home/src/scheduler/data_exporters/export_invoices_data.py:data_exporter:python:home/src/scheduler/data exporters/export invoices data": {"content": "from datetime import datetime, timezone\nfrom os import path\n\nimport pandas as pd\nfrom psycopg2.extras import execute_values\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\n\nCREATE TABLE IF NOT EXISTS raw.qb_invoices (\n    id TEXT PRIMARY KEY,\n    payload JSONB NOT NULL,\n    ingested_at_utc TIMESTAMPTZ NOT NULL,\n    extract_window_start_utc TIMESTAMPTZ NOT NULL,\n    extract_window_end_utc TIMESTAMPTZ NOT NULL,\n    page_number INTEGER NOT NULL,\n    page_size INTEGER NOT NULL,\n    request_payload JSONB NOT NULL\n);\n\"\"\"\n\nUPSERT_SQL = \"\"\"\nINSERT INTO raw.qb_invoices (\n    id,\n    payload,\n    ingested_at_utc,\n    extract_window_start_utc,\n    extract_window_end_utc,\n    page_number,\n    page_size,\n    request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE SET\n    payload = EXCLUDED.payload,\n    ingested_at_utc = EXCLUDED.ingested_at_utc,\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload;\n\"\"\"\n\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs) -> dict:\n    if df is None or df.empty:\n        log(\"Load: DataFrame vac\u00edo. No se carga nada.\")\n        return {\"rows_loaded\": 0}\n\n    required_cols = [\n        \"id\",\n        \"payload_json\",\n        \"ingested_at_utc\",\n        \"extract_window_start_utc\",\n        \"extract_window_end_utc\",\n        \"page_number\",\n        \"page_size\",\n        \"request_payload_json\",\n    ]\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Load: faltan columnas requeridas: {missing}\")\n\n    log(f\"Load: filas recibidas = {len(df)}\")\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n\n    rows = list(\n        df[\n            [\n                \"id\",\n                \"payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n                \"request_payload_json\",\n            ]\n        ].itertuples(index=False, name=None)\n    )\n\n    log(\"Load: abriendo conexi\u00f3n a Postgres...\")\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Algunas versiones exponen conn como loader.conn, otras diferente:\n        conn = getattr(loader, \"conn\", None) or getattr(loader, \"_conn\", None) or getattr(loader, \"_connection\", None)\n        if conn is None:\n            raise AttributeError(\"Load: no pude obtener la conexi\u00f3n desde el loader (conn/_conn/_connection).\")\n\n        log(\"Load: conexi\u00f3n abierta. Creando tabla si no existe...\")\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n\n            template = (\n                \"(\"\n                \"%s, \"\n                \"%s::jsonb, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s, \"\n                \"%s, \"\n                \"%s::jsonb\"\n                \")\"\n            )\n\n            log(\"Load: ejecutando upsert...\")\n            execute_values(cur, UPSERT_SQL, rows, template=template, page_size=1000)\n\n        conn.commit()\n        log(\"Load: commit realizado.\")\n\n    log(\"Load: terminado.\")\n    return {\"rows_loaded\": len(rows), \"target\": \"raw.qb_invoices\"}\n", "file_path": "/home/src/scheduler/data_exporters/export_invoices_data.py", "language": "python", "type": "data_exporter", "uuid": "export_invoices_data"}, "/home/src/scheduler/data_loaders/load_items_data.py:data_loader:python:home/src/scheduler/data loaders/load items data": {"content": "import time\nfrom datetime import datetime, timedelta, timezone\nfrom typing import List, Dict, Any, Tuple\n\nimport pandas as pd\nimport requests\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\n\nif \"data_loader\" not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n\nTOKEN_URL = \"https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer\"\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\ndef get_base_url() -> str:\n    env = get_secret_value(\"qbo_env\")\n    if str(env).lower() == \"sandbox\":\n        return \"https://sandbox-quickbooks.api.intuit.com\"\n    return \"https://quickbooks.api.intuit.com\"\n\n\ndef get_access_token() -> str:\n    client_id = get_secret_value(\"qbo_client_id\")\n    client_secret = get_secret_value(\"qbo_client_secret\")\n    refresh_token = get_secret_value(\"qbo_refresh_token\")\n\n    log(\"Pidiendo access token a Intuit...\")\n    response = requests.post(\n        TOKEN_URL,\n        auth=requests.auth.HTTPBasicAuth(client_id, client_secret),\n        data={\"grant_type\": \"refresh_token\", \"refresh_token\": refresh_token},\n        timeout=30,\n    )\n    response.raise_for_status()\n    log(\"Access token obtenido.\")\n    return response.json()[\"access_token\"]\n\n\ndef request_get_with_retries(\n    url: str,\n    headers: Dict[str, str],\n    params: Dict[str, Any],\n    max_retries: int = 5\n) -> Dict[str, Any]:\n    for attempt in range(max_retries):\n        r = requests.get(url, headers=headers, params=params, timeout=60)\n\n        if r.status_code in (429, 500, 502, 503, 504):\n            wait_s = 2 ** attempt\n            log(f\"HTTP {r.status_code}. Reintento {attempt + 1}/{max_retries} en {wait_s}s...\")\n            time.sleep(wait_s)\n            continue\n\n        r.raise_for_status()\n        return r.json()\n\n    raise RuntimeError(\"QuickBooks request fall\u00f3 tras varios reintentos.\")\n\n\ndef format_qb_datetime(dt: datetime) -> str:\n    return dt.astimezone(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n\ndef split_date_range(start_utc: datetime, end_utc: datetime, chunk_days: int) -> List[Tuple[datetime, datetime]]:\n    windows = []\n    current = start_utc\n    while current < end_utc:\n        nxt = min(current + timedelta(days=chunk_days), end_utc)\n        windows.append((current, nxt))\n        current = nxt\n    return windows\n\n\ndef fetch_items_window(\n    realm_id: str,\n    base_url: str,\n    access_token: str,\n    start_dt: datetime,\n    end_dt: datetime,\n    minor_version: int = 75,\n    page_size: int = 1000,\n) -> List[Dict[str, Any]]:\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Accept\": \"application/json\",\n    }\n\n    start_str = format_qb_datetime(start_dt)\n    end_str = format_qb_datetime(end_dt)\n\n    log(f\"Extrayendo ventana: {start_str} -> {end_str}\")\n\n    start_position = 1\n    page_number = 1\n    rows: List[Dict[str, Any]] = []\n\n    while True:\n        query = (\n            \"SELECT * FROM Item \"\n            f\"WHERE MetaData.LastUpdatedTime >= '{start_str}' \"\n            f\"AND MetaData.LastUpdatedTime < '{end_str}' \"\n            f\"STARTPOSITION {start_position} MAXRESULTS {page_size}\"\n        )\n\n        params = {\"query\": query, \"minorversion\": minor_version}\n        data = request_get_with_retries(url, headers, params)\n\n        if \"Fault\" in data:\n            log(\"QuickBooks respondi\u00f3 con 'Fault'. Se omite esta ventana.\")\n            break\n\n        items = data.get(\"QueryResponse\", {}).get(\"Item\", [])\n        if not items:\n            if page_number == 1:\n                log(\"Ventana sin resultados.\")\n            break\n\n        log(f\"  P\u00e1gina {page_number}: {len(items)} items\")\n\n        request_payload = {\"query\": query, \"minorversion\": minor_version}\n\n        for c in items:\n            rows.append(\n                {\n                    \"id\": str(c.get(\"Id\")) if c.get(\"Id\") is not None else None,\n                    \"payload\": c,\n                    \"extract_window_start_utc\": start_str,\n                    \"extract_window_end_utc\": end_str,\n                    \"page_number\": page_number,\n                    \"page_size\": page_size,\n                    \"request_payload\": request_payload,\n                }\n            )\n\n        if len(items) < page_size:\n            break\n\n        start_position += page_size\n        page_number += 1\n        time.sleep(0.3)\n\n    log(f\"Ventana terminada. Filas acumuladas: {len(rows)}\")\n    return rows\n\n\n@data_loader\ndef load_data(*args, **kwargs) -> pd.DataFrame:\n    fecha_inicio = kwargs.get(\"fecha_inicio\")\n    fecha_fin = kwargs.get(\"fecha_fin\")\n    if not fecha_inicio or not fecha_fin:\n        raise ValueError(\"Debes pasar fecha_inicio y fecha_fin, ej: '2024-01-01T00:00:00Z'.\")\n\n    chunk_days = int(kwargs.get(\"chunk_days\", 30))\n\n    start_utc = datetime.fromisoformat(fecha_inicio.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n    end_utc = datetime.fromisoformat(fecha_fin.replace(\"Z\", \"+00:00\")).astimezone(timezone.utc)\n\n    log(\"Inicio Extract Items\")\n    log(f\"Rango: {fecha_inicio} -> {fecha_fin} | chunk_days={chunk_days}\")\n\n    realm_id = get_secret_value(\"qbo_realm_id\")\n    base_url = get_base_url()\n    access_token = get_access_token()\n\n    windows = split_date_range(start_utc, end_utc, chunk_days)\n    log(f\"Total ventanas a procesar: {len(windows)}\")\n\n    all_rows: List[Dict[str, Any]] = []\n    for i, (w_start, w_end) in enumerate(windows, start=1):\n        log(f\"Procesando ventana {i}/{len(windows)}\")\n        all_rows.extend(\n            fetch_items_window(\n                realm_id=realm_id,\n                base_url=base_url,\n                access_token=access_token,\n                start_dt=w_start,\n                end_dt=w_end,\n            )\n        )\n\n    df = pd.DataFrame(all_rows)\n    log(f\"Extract terminado. Total filas: {len(df)}\")\n    return df\n", "file_path": "/home/src/scheduler/data_loaders/load_items_data.py", "language": "python", "type": "data_loader", "uuid": "load_items_data"}, "/home/src/scheduler/transformers/transform_items_data.py:transformer:python:home/src/scheduler/transformers/transform items data": {"content": "import json\nfrom datetime import datetime, timezone\n\nimport pandas as pd\n\nif \"transformer\" not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\n@transformer\ndef transform(df: pd.DataFrame, *args, **kwargs) -> pd.DataFrame:\n    if df is None or df.empty:\n        log(\"Transform: DataFrame vac\u00edo.\")\n        return pd.DataFrame(\n            columns=[\n                \"id\",\n                \"payload_json\",\n                \"request_payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n            ]\n        )\n\n    log(f\"Transform: filas recibidas = {len(df)}\")\n\n    df = df.copy()\n\n    # id obligatorio\n    df = df[df[\"id\"].notna()]\n    df[\"id\"] = df[\"id\"].astype(str)\n\n    # ingesta (UTC)\n    df[\"ingested_at_utc\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n    # JSON listo para jsonb\n    df[\"payload_json\"] = df[\"payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n    df[\"request_payload_json\"] = df[\"request_payload\"].apply(\n        lambda x: json.dumps(x if x is not None else {}, ensure_ascii=False)\n    )\n\n    # asegurar tipos\n    df[\"page_number\"] = pd.to_numeric(df[\"page_number\"], errors=\"coerce\").fillna(0).astype(int)\n    df[\"page_size\"] = pd.to_numeric(df[\"page_size\"], errors=\"coerce\").fillna(0).astype(int)\n\n    out = df[\n        [\n            \"id\",\n            \"payload_json\",\n            \"request_payload_json\",\n            \"ingested_at_utc\",\n            \"extract_window_start_utc\",\n            \"extract_window_end_utc\",\n            \"page_number\",\n            \"page_size\",\n        ]\n    ].reset_index(drop=True)\n\n    log(f\"Transform: filas de salida = {len(out)}\")\n    return out\n", "file_path": "/home/src/scheduler/transformers/transform_items_data.py", "language": "python", "type": "transformer", "uuid": "transform_items_data"}, "/home/src/scheduler/data_exporters/export_items_data.py:data_exporter:python:home/src/scheduler/data exporters/export items data": {"content": "from datetime import datetime, timezone\nfrom os import path\n\nimport pandas as pd\nfrom psycopg2.extras import execute_values\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\ndef log(msg: str) -> None:\n    ts = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(f\"[{ts} UTC] {msg}\")\n\n\nDDL = \"\"\"\nCREATE SCHEMA IF NOT EXISTS raw;\n\nCREATE TABLE IF NOT EXISTS raw.qb_items (\n    id TEXT PRIMARY KEY,\n    payload JSONB NOT NULL,\n    ingested_at_utc TIMESTAMPTZ NOT NULL,\n    extract_window_start_utc TIMESTAMPTZ NOT NULL,\n    extract_window_end_utc TIMESTAMPTZ NOT NULL,\n    page_number INTEGER NOT NULL,\n    page_size INTEGER NOT NULL,\n    request_payload JSONB NOT NULL\n);\n\"\"\"\n\nUPSERT_SQL = \"\"\"\nINSERT INTO raw.qb_items (\n    id,\n    payload,\n    ingested_at_utc,\n    extract_window_start_utc,\n    extract_window_end_utc,\n    page_number,\n    page_size,\n    request_payload\n)\nVALUES %s\nON CONFLICT (id) DO UPDATE SET\n    payload = EXCLUDED.payload,\n    ingested_at_utc = EXCLUDED.ingested_at_utc,\n    extract_window_start_utc = EXCLUDED.extract_window_start_utc,\n    extract_window_end_utc = EXCLUDED.extract_window_end_utc,\n    page_number = EXCLUDED.page_number,\n    page_size = EXCLUDED.page_size,\n    request_payload = EXCLUDED.request_payload;\n\"\"\"\n\n\n@data_exporter\ndef export_data_to_postgres(df: pd.DataFrame, **kwargs) -> dict:\n    if df is None or df.empty:\n        log(\"Load: DataFrame vac\u00edo. No se carga nada.\")\n        return {\"rows_loaded\": 0}\n\n    required_cols = [\n        \"id\",\n        \"payload_json\",\n        \"ingested_at_utc\",\n        \"extract_window_start_utc\",\n        \"extract_window_end_utc\",\n        \"page_number\",\n        \"page_size\",\n        \"request_payload_json\",\n    ]\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Load: faltan columnas requeridas: {missing}\")\n\n    log(f\"Load: filas recibidas = {len(df)}\")\n\n    config_path = path.join(get_repo_path(), \"io_config.yaml\")\n    config_profile = \"default\"\n\n    rows = list(\n        df[\n            [\n                \"id\",\n                \"payload_json\",\n                \"ingested_at_utc\",\n                \"extract_window_start_utc\",\n                \"extract_window_end_utc\",\n                \"page_number\",\n                \"page_size\",\n                \"request_payload_json\",\n            ]\n        ].itertuples(index=False, name=None)\n    )\n\n    log(\"Load: abriendo conexi\u00f3n a Postgres...\")\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Algunas versiones exponen conn como loader.conn, otras diferente:\n        conn = getattr(loader, \"conn\", None) or getattr(loader, \"_conn\", None) or getattr(loader, \"_connection\", None)\n        if conn is None:\n            raise AttributeError(\"Load: no pude obtener la conexi\u00f3n desde el loader (conn/_conn/_connection).\")\n\n        log(\"Load: conexi\u00f3n abierta. Creando tabla si no existe...\")\n        with conn.cursor() as cur:\n            cur.execute(DDL)\n\n            template = (\n                \"(\"\n                \"%s, \"\n                \"%s::jsonb, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s::timestamptz, \"\n                \"%s, \"\n                \"%s, \"\n                \"%s::jsonb\"\n                \")\"\n            )\n\n            log(\"Load: ejecutando upsert...\")\n            execute_values(cur, UPSERT_SQL, rows, template=template, page_size=1000)\n\n        conn.commit()\n        log(\"Load: commit realizado.\")\n\n    log(\"Load: terminado.\")\n    return {\"rows_loaded\": len(rows), \"target\": \"raw.qb_items\"}\n", "file_path": "/home/src/scheduler/data_exporters/export_items_data.py", "language": "python", "type": "data_exporter", "uuid": "export_items_data"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}